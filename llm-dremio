# client/llm.py
import os
from langchain_openai import ChatOpenAI

def get_llama_llm():
    return ChatOpenAI(
        model=os.environ.get("LLM_MODEL", "llama-3.1-70b"),
        base_url=os.environ["OPENAI_BASE_URL"],  # Llama gateway
        api_key=os.environ.get("OPENAI_API_KEY", "dummy"),
        default_headers={
            "X-API-Key": os.environ["X_API_KEY"],
            "X-Auth-Groups": os.environ["X_AUTH_GROUPS"],
        },
        temperature=0,
    )

langchain-mcp-adapters>=0.1.7
# client/mcp_tools.py
from langchain_mcp_adapters import MCPToolProvider

def get_dremio_tools():
    provider = MCPToolProvider(
        command=[
            "dremio-mcp-server",
            "run"
        ]
    )
    return provider.get_tools()


# client/agent.py
from langchain.agents import create_tool_calling_agent, AgentExecutor
from langchain.prompts import ChatPromptTemplate

from llm import get_llama_llm
from mcp_tools import get_dremio_tools

def build_agent():
    llm = get_llama_llm()
    tools = get_dremio_tools()

    prompt = ChatPromptTemplate.from_messages([
        ("system", 
         "You are a Dremio data analyst. "
         "Use tools to discover schemas and generate valid Dremio SQL. "
         "Never hallucinate tables or columns."),
        ("human", "{input}"),
        ("placeholder", "{agent_scratchpad}")
    ])

    agent = create_tool_calling_agent(
        llm=llm,
        tools=tools,
        prompt=prompt
    )

    return AgentExecutor(
        agent=agent,
        tools=tools,
        verbose=True
    )


 


# client/graph_agent.py
from langgraph.graph import StateGraph, END
from langchain_core.messages import HumanMessage
from typing import TypedDict, List

from llm import get_llama_llm
from mcp_tools import get_dremio_tools


class AgentState(TypedDict):
    messages: List


def build_graph():
    llm = get_llama_llm()
    tools = get_dremio_tools()
    llm = llm.bind_tools(tools)

    def call_llm(state: AgentState):
        response = llm.invoke(state["messages"])
        return {"messages": state["messages"] + [response]}

    graph = StateGraph(AgentState)
    graph.add_node("llm", call_llm)
    graph.set_entry_point("llm")
    graph.add_edge("llm", END)

    return graph.compile()


# client/run.py
from langchain_core.messages import HumanMessage
from graph_agent import build_graph

app = build_graph()

while True:
    q = input("\nAsk Dremio> ")
    if q.lower() in ("exit", "quit"):
        break

    result = app.invoke({
        "messages": [HumanMessage(content=q)]
    })

    print(result["messages"][-1].content)


export OPENAI_BASE_URL="https://llama.company.internal/v1"
export OPENAI_API_KEY="dummy"
export LLM_MODEL="llama-3.1-70b"
export X_API_KEY="real-api-key"
export X_AUTH_GROUPS="finance,analytics"


source .venv/bin/activate
python client/run.py
